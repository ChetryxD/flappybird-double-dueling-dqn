# ğŸ¦ Flappy Bird AI â€” Double Dueling DQN (PyTorch)

> A from-scratch implementation of a **Double Dueling Deep Q-Network (DQN)** trained to master Flappy Bird using value-based reinforcement learning.

![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![License](https://img.shields.io/badge/License-MIT-lightgrey)
![RL](https://img.shields.io/badge/Reinforcement-Learning-green)

---

## ğŸ® Demo

![flappybirds](https://github.com/user-attachments/assets/658bd3f9-401e-4e47-a63d-d14c9ca817a5)


---

## ğŸ“Œ Overview

This project implements a **Double Dueling Deep Q-Network (DQN)** to solve Flappy Bird in a sparse-reward environment.

To achieve stable long-horizon learning, the agent integrates:

- **Experience Replay**
- **Target Network Synchronization**
- **Double DQN** (reduces Q-value overestimation)
- **Dueling Architecture** (separates state-value and advantage streams)
- **Huber Loss (SmoothL1Loss)** for stable updates

The result is a stable agent capable of consistently passing dozens of pipes.

---

## ğŸš€ Performance

| Metric | Value |
|--------|--------|
| Peak Score | **80â€“120+ pipes** |
| Consistent Average | **40â€“70 pipes** |
| Convergence | ~300k episodes |
| Training Time | ~3â€“4 hours (CPU) |

âœ… Stable learning curve  
âœ… Reduced Q-value oscillations  
âœ… Strong generalization beyond early episodes  

---

## ğŸ“ˆ Training Curve

<img width="788" height="587" alt="image" src="https://github.com/user-attachments/assets/f4a73851-ca98-4ed6-a24f-1e831927a083" />

---

## ğŸ§  Algorithmic Architecture

| Technique | Purpose |
|------------|----------|
| Experience Replay | Breaks temporal correlation between samples |
| Target Network | Stabilizes Q-learning updates |
| Double DQN | Mitigates overestimation bias |
| Dueling DQN | Separates state-value from action advantage |
| Huber Loss | Prevents gradient instability |

### Dueling Decomposition

The Q-value is computed as:

```python
Q(s, a) = V(s) + (A(s, a) - mean(A(s, Â·)))
ğŸ“‚ Project Structure

dqn_pytorch/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ dqn.py
â”‚   â””â”€â”€ experience_replay.py
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ hyperparameters.yml
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ learning_curve.png
â”‚   â””â”€â”€ flappybird_demo.gif
â”‚
â”œâ”€â”€ runs/               # training outputs (gitignored)
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Installation

git clone https://github.com/ChetryxD/flappybird-double-dueling-dqn.git
cd flappybird-double-dueling-dqn

python -m venv venv
venv\Scripts\activate

pip install -r requirements.txt

ğŸ‹ï¸ Training

python -m src.agent --train

ğŸ® Run Trained Agent

python -m src.agent

ğŸ§ª Hyperparameters
learning_rate: 0.00015
gamma: 0.99
replay_memory_size: 100000
batch_size: 64
epsilon_decay: 0.999995
double_dqn: true
dueling_dqn: true

ğŸ›  Engineering Decisions

Forced CPU execution (stable for small networks)

Reduced network size to prevent overfitting

Increased target sync interval

Lowered learning rate for smoother convergence

Switched from MSE to Huber Loss

ğŸ“š Key Learnings

Stabilization techniques matter more than network depth

Double DQN reduces oscillatory Q-values

Dueling improves sparse reward learning

Hyperparameter tuning dominates performance

ğŸ”® Future Improvements

Prioritized Experience Replay

Noisy Networks

Multi-step returns

PPO baseline comparison

Model checkpoint evaluation pipeline
